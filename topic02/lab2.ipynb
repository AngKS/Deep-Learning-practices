{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(42, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-04 12:23:26.031640: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Hello World (with Eager Execution)\n",
    "a = tf.constant(10)  # create tensor for a and b\n",
    "b = tf.constant(32)\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Hello World (with Graph Execution)\n",
    "tf.compat.v1.disable_eager_execution()  # this is to disable eager execution mode\n",
    "\n",
    "a = tf.constant(10)  # create tensor for a and b\n",
    "b = tf.constant(32)\n",
    "sess = tf.compat.v1.Session()\n",
    "print(sess.run(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                180       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                252       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 445\n",
      "Trainable params: 445\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Keras Hello World\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# create model\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(20, input_dim=8, activation='relu'),\n",
    "        Dense(12, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "        ]\n",
    ")\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear Regression with Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 0 W: [-0.30375975] b: [0.65768325]\n",
      "Steps: 20 W: [-0.01011735] b: [0.35469866]\n",
      "Steps: 40 W: [0.07399908] b: [0.31291547]\n",
      "Steps: 60 W: [0.09386067] b: [0.30304962]\n",
      "Steps: 80 W: [0.09855038] b: [0.30072007]\n",
      "Steps: 100 W: [0.09965771] b: [0.30017003]\n",
      "Steps: 120 W: [0.09991919] b: [0.30004016]\n",
      "Steps: 140 W: [0.09998091] b: [0.3000095]\n",
      "Steps: 160 W: [0.09999549] b: [0.30000225]\n",
      "Steps: 180 W: [0.09999895] b: [0.30000052]\n",
      "Steps: 200 W: [0.09999976] b: [0.30000013]\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression using Tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.compat.v1.disable_eager_execution()\n",
    " \n",
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    " \n",
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but Tensorflow will\n",
    "# figure that out for us.)\n",
    "W = tf.Variable(tf.random.uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b\n",
    "\n",
    "\n",
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.5)\n",
    "train = optimizer.minimize(loss)\n",
    " \n",
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    " \n",
    "# Launch the graph.\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init)\n",
    " \n",
    "# Fit the line.\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print('Steps:', step, 'W:', sess.run(W), 'b:', sess.run(b))\n",
    " \n",
    "# Learns best fit is W: [0.1], b: [0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use Tensorflow for iris data classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpz5jt2_3e\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpz5jt2_3e', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py:63: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/ipykernel_87248/3630313064.py:24: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:60: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/keras/optimizer_v2/adagrad.py:83: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:907: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpz5jt2_3e/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 1.0045825, step = 0\n",
      "INFO:tensorflow:global_step/sec: 611.778\n",
      "INFO:tensorflow:loss = 0.8538231, step = 100 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 838.623\n",
      "INFO:tensorflow:loss = 0.77141553, step = 200 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 878.157\n",
      "INFO:tensorflow:loss = 0.7177267, step = 300 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 1111.17\n",
      "INFO:tensorflow:loss = 0.6755354, step = 400 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 919.259\n",
      "INFO:tensorflow:loss = 0.64023226, step = 500 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 823.669\n",
      "INFO:tensorflow:loss = 0.61024594, step = 600 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 668.83\n",
      "INFO:tensorflow:loss = 0.58423716, step = 700 (0.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 705.697\n",
      "INFO:tensorflow:loss = 0.56131727, step = 800 (0.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 877.376\n",
      "INFO:tensorflow:loss = 0.5410444, step = 900 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 1130.02\n",
      "INFO:tensorflow:loss = 0.5229178, step = 1000 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1004.05\n",
      "INFO:tensorflow:loss = 0.5065345, step = 1100 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 814.254\n",
      "INFO:tensorflow:loss = 0.49162737, step = 1200 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 816.658\n",
      "INFO:tensorflow:loss = 0.47792235, step = 1300 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 809.352\n",
      "INFO:tensorflow:loss = 0.4653022, step = 1400 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 1092.28\n",
      "INFO:tensorflow:loss = 0.453789, step = 1500 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1222.97\n",
      "INFO:tensorflow:loss = 0.44329563, step = 1600 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 975.363\n",
      "INFO:tensorflow:loss = 0.43356636, step = 1700 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 776.048\n",
      "INFO:tensorflow:loss = 0.4245294, step = 1800 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 862.486\n",
      "INFO:tensorflow:loss = 0.41609994, step = 1900 (0.116 sec)\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1921 vs previous value: 1921. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2000...\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpz5jt2_3e/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2000...\n",
      "INFO:tensorflow:Loss for final step: 0.6445063.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x7f9555b441f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-04T12:23:32\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpz5jt2_3e/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 0.29913s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-04-12:23:32\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.96666664, average_loss = 0.65028197, global_step = 2000, loss = 0.65028197\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpz5jt2_3e/model.ckpt-2000\n",
      "Accuracy (tensorflow): 0.966667\n"
     ]
    }
   ],
   "source": [
    "# Using TensorFLow for iris data classification\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "# Split dataset into train / test\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "      x, y, test_size=0.2, random_state=42)\n",
    "# Scale data (training set) to 0 mean and unit standard deviation.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "# Set the feature columns\n",
    "# We use the shape value to define x as 4 column numeric feature input (0..3)\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=4)]\n",
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "      feature_columns=feature_columns, hidden_units=[10, 20, 10], n_classes=3)\n",
    "# Train.\n",
    "train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(x={\"x\": x_train},\n",
    "                                                    y=y_train,\n",
    "                                                    batch_size=6,\n",
    "                                                    shuffle=False,\n",
    "                                                    num_epochs=100)\n",
    "classifier.train(input_fn=train_input_fn)\n",
    "# Eval.\n",
    "test_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(x={\"x\": x_test},\n",
    "                                                   y=y_test,\n",
    "                                                   batch_size=1,\n",
    "                                                   shuffle=False)\n",
    "scores = classifier.evaluate(input_fn=test_input_fn)\n",
    "print('Accuracy (tensorflow): {0:f}'.format(scores['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Keras Sequential Model**\n",
    "2 ways to create Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# create model\n",
    "model = Sequential([\n",
    "\tDense(12, input_dim=8, activation='relu'),\n",
    "\tDense(8, activation='relu'),\n",
    "\tDense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use Keras for iris data classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 0s 668us/sample - loss: 0.6413 - accuracy: 0.5250\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 86us/sample - loss: 0.5799 - accuracy: 0.7875\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 77us/sample - loss: 0.5238 - accuracy: 0.9250\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 84us/sample - loss: 0.4713 - accuracy: 0.9750\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 89us/sample - loss: 0.4228 - accuracy: 0.9875\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 104us/sample - loss: 0.3760 - accuracy: 0.9875\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 83us/sample - loss: 0.3316 - accuracy: 0.9875\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.2882 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.2464 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.2055 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f953955f9a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy: 100.00%\n",
      "[0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# loading and pre-processing of the data\n",
    "# We use the 2 class version of iris data set\n",
    "iris = pd.read_csv(\"iris.csv\")\n",
    "x = np.array(iris.drop(\"Class\",axis=1))\n",
    "y = np.array(iris[\"Class\"])\n",
    "# Split dataset into train / test\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "      x, y, test_size=0.2, random_state=42)\n",
    "# Scale data (training set) to 0 mean and unit standard deviation.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "# training the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=10)\n",
    "# eval model\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# calculate predictions\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1540c76d30b1af6ad410dda6d61bebe77544e2f3c1a272e4083818637a64b89a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tf2.6': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
