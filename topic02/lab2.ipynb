{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(42, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 08:14:58.850142: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Hello World (with Eager Execution)\n",
    "a = tf.constant(10)  # create tensor for a and b\n",
    "b = tf.constant(32)\n",
    "print(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Hello World (with Graph Execution)\n",
    "tf.compat.v1.disable_eager_execution()  # this is to disable eager execution mode\n",
    "\n",
    "a = tf.constant(10)  # create tensor for a and b\n",
    "b = tf.constant(32)\n",
    "sess = tf.compat.v1.Session()\n",
    "print(sess.run(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Keras Hello World\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# create model\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(12, input_dim=8, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "        ]\n",
    ")\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear Regression with Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 0 W: [-0.35102957] b: [0.8287442]\n",
      "Steps: 20 W: [-0.05092867] b: [0.3869753]\n",
      "Steps: 40 W: [0.05984777] b: [0.32313845]\n",
      "Steps: 60 W: [0.08931813] b: [0.30615562]\n",
      "Steps: 80 W: [0.09715825] b: [0.30163762]\n",
      "Steps: 100 W: [0.099244] b: [0.30043566]\n",
      "Steps: 120 W: [0.09979887] b: [0.3001159]\n",
      "Steps: 140 W: [0.09994651] b: [0.30003086]\n",
      "Steps: 160 W: [0.09998578] b: [0.3000082]\n",
      "Steps: 180 W: [0.09999622] b: [0.3000022]\n",
      "Steps: 200 W: [0.099999] b: [0.30000058]\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression using Tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.compat.v1.disable_eager_execution()\n",
    " \n",
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    " \n",
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but Tensorflow will\n",
    "# figure that out for us.)\n",
    "W = tf.Variable(tf.random.uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b\n",
    "\n",
    "\n",
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.5)\n",
    "train = optimizer.minimize(loss)\n",
    " \n",
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    " \n",
    "# Launch the graph.\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init)\n",
    " \n",
    "# Fit the line.\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print('Steps:', step, 'W:', sess.run(W), 'b:', sess.run(b))\n",
    " \n",
    "# Learns best fit is W: [0.1], b: [0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use Tensorflow for iris data classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpzy_nzggp\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpzy_nzggp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py:63: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/ipykernel_7542/3630313064.py:24: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:60: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/keras/optimizer_v2/adagrad.py:83: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:907: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpzy_nzggp/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 1.076245, step = 0\n",
      "INFO:tensorflow:global_step/sec: 892.084\n",
      "INFO:tensorflow:loss = 1.0077077, step = 100 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 1117.87\n",
      "INFO:tensorflow:loss = 0.9766278, step = 200 (0.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 1352.49\n",
      "INFO:tensorflow:loss = 0.95167667, step = 300 (0.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 974.735\n",
      "INFO:tensorflow:loss = 0.9303598, step = 400 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 1414.79\n",
      "INFO:tensorflow:loss = 0.9107657, step = 500 (0.071 sec)\n",
      "INFO:tensorflow:global_step/sec: 1515.06\n",
      "INFO:tensorflow:loss = 0.8922562, step = 600 (0.066 sec)\n",
      "INFO:tensorflow:global_step/sec: 1493.68\n",
      "INFO:tensorflow:loss = 0.8748085, step = 700 (0.067 sec)\n",
      "INFO:tensorflow:global_step/sec: 1426.47\n",
      "INFO:tensorflow:loss = 0.85842395, step = 800 (0.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 1367.28\n",
      "INFO:tensorflow:loss = 0.84310794, step = 900 (0.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 1258\n",
      "INFO:tensorflow:loss = 0.82876045, step = 1000 (0.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 1468.67\n",
      "INFO:tensorflow:loss = 0.8152358, step = 1100 (0.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 1433.03\n",
      "INFO:tensorflow:loss = 0.802337, step = 1200 (0.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 1473.08\n",
      "INFO:tensorflow:loss = 0.7894733, step = 1300 (0.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 1504.21\n",
      "INFO:tensorflow:loss = 0.77635306, step = 1400 (0.066 sec)\n",
      "INFO:tensorflow:global_step/sec: 1478.9\n",
      "INFO:tensorflow:loss = 0.76378727, step = 1500 (0.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 1514.97\n",
      "INFO:tensorflow:loss = 0.7513797, step = 1600 (0.066 sec)\n",
      "INFO:tensorflow:global_step/sec: 1476.51\n",
      "INFO:tensorflow:loss = 0.73932976, step = 1700 (0.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 1329.92\n",
      "INFO:tensorflow:loss = 0.7277389, step = 1800 (0.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 1774.82\n",
      "INFO:tensorflow:loss = 0.71663827, step = 1900 (0.056 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2000...\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpzy_nzggp/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2000...\n",
      "INFO:tensorflow:Loss for final step: 0.7844743.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x7fe7e344dee0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-01T08:15:04\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpzy_nzggp/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 0.16303s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-01-08:15:04\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.76666665, average_loss = 0.6649304, global_step = 2000, loss = 0.6649304\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/tmpzy_nzggp/model.ckpt-2000\n",
      "Accuracy (tensorflow): 0.766667\n"
     ]
    }
   ],
   "source": [
    "# Using TensorFLow for iris data classification\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "# Split dataset into train / test\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "      x, y, test_size=0.2, random_state=42)\n",
    "# Scale data (training set) to 0 mean and unit standard deviation.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "# Set the feature columns\n",
    "# We use the shape value to define x as 4 column numeric feature input (0..3)\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=4)]\n",
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "      feature_columns=feature_columns, hidden_units=[10, 20, 10], n_classes=3)\n",
    "# Train.\n",
    "train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(x={\"x\": x_train},\n",
    "                                                    y=y_train,\n",
    "                                                    batch_size=6,\n",
    "                                                    shuffle=False,\n",
    "                                                    num_epochs=100)\n",
    "classifier.train(input_fn=train_input_fn)\n",
    "# Eval.\n",
    "test_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(x={\"x\": x_test},\n",
    "                                                   y=y_test,\n",
    "                                                   batch_size=1,\n",
    "                                                   shuffle=False)\n",
    "scores = classifier.evaluate(input_fn=test_input_fn)\n",
    "print('Accuracy (tensorflow): {0:f}'.format(scores['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Keras Sequential Model**\n",
    "2 ways to create Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# create model\n",
    "model = Sequential([\n",
    "\tDense(12, input_dim=8, activation='relu'),\n",
    "\tDense(8, activation='relu'),\n",
    "\tDense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use Keras for iris data classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 0s 651us/sample - loss: 0.6654 - accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 0s 76us/sample - loss: 0.5839 - accuracy: 0.9250\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 0s 69us/sample - loss: 0.5141 - accuracy: 0.9875\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 0s 71us/sample - loss: 0.4508 - accuracy: 0.9875\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 0s 76us/sample - loss: 0.3885 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 0s 75us/sample - loss: 0.3290 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 0s 70us/sample - loss: 0.2739 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 0s 71us/sample - loss: 0.2236 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 0s 79us/sample - loss: 0.1802 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 0s 69us/sample - loss: 0.1447 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7c61dd940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy: 100.00%\n",
      "[0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angks/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# loading and pre-processing of the data\n",
    "# We use the 2 class version of iris data set\n",
    "iris = pd.read_csv(\"iris.csv\")\n",
    "x = np.array(iris.drop(\"Class\",axis=1))\n",
    "y = np.array(iris[\"Class\"])\n",
    "# Split dataset into train / test\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "      x, y, test_size=0.2, random_state=42)\n",
    "# Scale data (training set) to 0 mean and unit standard deviation.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "# training the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=10)\n",
    "# eval model\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "# calculate predictions\n",
    "predictions = model.predict(x_test)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1540c76d30b1af6ad410dda6d61bebe77544e2f3c1a272e4083818637a64b89a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tf2.6': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
