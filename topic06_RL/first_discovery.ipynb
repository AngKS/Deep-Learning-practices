{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rocket trajectory optimization is a classic topic in Optimal Control.\n",
    "\n",
    "According to Pontryagin's maximum principle it's optimal to fire engine full throttle or\n",
    "turn it off. That's the reason this environment is OK to have discreet actions (engine on or off).\n",
    "\n",
    "The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector.\n",
    "Reward for moving from the top of the screen to the landing pad and zero speed is about 100..140 points.\n",
    "If the lander moves away from the landing pad it loses reward. The episode finishes if the lander crashes or\n",
    "comes to rest, receiving an additional -100 or +100 points. Each leg with ground contact is +10 points.\n",
    "Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame.\n",
    "Solved is 200 points.\n",
    "\n",
    "Landing outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land\n",
    "on its first attempt. Please see the source code for details.\n",
    "\n",
    "To see a heuristic landing, run:\n",
    "\n",
    "python gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "To play yourself, run:\n",
    "\n",
    "python examples/agents/keyboard_agent.py LunarLander-v2\n",
    "\n",
    "Created by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import Box2D\n",
    "from Box2D.b2 import (\n",
    "    edgeShape,\n",
    "    circleShape,\n",
    "    fixtureDef,\n",
    "    polygonShape,\n",
    "    revoluteJointDef,\n",
    "    contactListener,\n",
    ")\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces\n",
    "from gym.utils import seeding, EzPickle\n",
    "\n",
    "FPS = 50\n",
    "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
    "\n",
    "MAIN_ENGINE_POWER = 13.0\n",
    "SIDE_ENGINE_POWER = 0.6\n",
    "\n",
    "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
    "\n",
    "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
    "LEG_AWAY = 20\n",
    "LEG_DOWN = 18\n",
    "LEG_W, LEG_H = 2, 8\n",
    "LEG_SPRING_TORQUE = 40\n",
    "\n",
    "SIDE_ENGINE_HEIGHT = 14.0\n",
    "SIDE_ENGINE_AWAY = 12.0\n",
    "\n",
    "VIEWPORT_W = 600\n",
    "VIEWPORT_H = 400\n",
    "\n",
    "\n",
    "class ContactDetector(contactListener):\n",
    "    def __init__(self, env):\n",
    "        contactListener.__init__(self)\n",
    "        self.env = env\n",
    "\n",
    "    def BeginContact(self, contact):\n",
    "        if (\n",
    "            self.env.lander == contact.fixtureA.body\n",
    "            or self.env.lander == contact.fixtureB.body\n",
    "        ):\n",
    "            self.env.game_over = True\n",
    "        for i in range(2):\n",
    "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
    "                self.env.legs[i].ground_contact = True\n",
    "\n",
    "    def EndContact(self, contact):\n",
    "        for i in range(2):\n",
    "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
    "                self.env.legs[i].ground_contact = False\n",
    "\n",
    "\n",
    "class LunarLander(gym.Env, EzPickle):\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": FPS}\n",
    "\n",
    "    def __init__(self, continuous: bool = False):\n",
    "        EzPickle.__init__(self)\n",
    "        self.viewer = None\n",
    "\n",
    "        self.world = Box2D.b2World()\n",
    "        self.moon = None\n",
    "        self.lander = None\n",
    "        self.particles = []\n",
    "\n",
    "        self.prev_reward = None\n",
    "\n",
    "        self.continuous = continuous\n",
    "\n",
    "        # useful range is -1 .. +1, but spikes can be higher\n",
    "        self.observation_space = spaces.Box(\n",
    "            -np.inf, np.inf, shape=(8,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        if self.continuous:\n",
    "            # Action is two floats [main engine, left-right engines].\n",
    "            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n",
    "            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
    "            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)\n",
    "        else:\n",
    "            # Nop, fire left engine, main engine, right engine\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "\n",
    "    def _destroy(self):\n",
    "        if not self.moon:\n",
    "            return\n",
    "        self.world.contactListener = None\n",
    "        self._clean_particles(True)\n",
    "        self.world.DestroyBody(self.moon)\n",
    "        self.moon = None\n",
    "        self.world.DestroyBody(self.lander)\n",
    "        self.lander = None\n",
    "        self.world.DestroyBody(self.legs[0])\n",
    "        self.world.DestroyBody(self.legs[1])\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None):\n",
    "        super().reset(seed=seed)\n",
    "        self._destroy()\n",
    "        self.world.contactListener_keepref = ContactDetector(self)\n",
    "        self.world.contactListener = self.world.contactListener_keepref\n",
    "        self.game_over = False\n",
    "        self.prev_shaping = None\n",
    "\n",
    "        W = VIEWPORT_W / SCALE\n",
    "        H = VIEWPORT_H / SCALE\n",
    "\n",
    "        # terrain\n",
    "        CHUNKS = 11\n",
    "        height = self.np_random.uniform(0, H / 2, size=(CHUNKS + 1,))\n",
    "        chunk_x = [W / (CHUNKS - 1) * i for i in range(CHUNKS)]\n",
    "        self.helipad_x1 = chunk_x[CHUNKS // 2 - 1]\n",
    "        self.helipad_x2 = chunk_x[CHUNKS // 2 + 1]\n",
    "        self.helipad_y = H / 4\n",
    "        height[CHUNKS // 2 - 2] = self.helipad_y\n",
    "        height[CHUNKS // 2 - 1] = self.helipad_y\n",
    "        height[CHUNKS // 2 + 0] = self.helipad_y\n",
    "        height[CHUNKS // 2 + 1] = self.helipad_y\n",
    "        height[CHUNKS // 2 + 2] = self.helipad_y\n",
    "        smooth_y = [\n",
    "            0.33 * (height[i - 1] + height[i + 0] + height[i + 1])\n",
    "            for i in range(CHUNKS)\n",
    "        ]\n",
    "\n",
    "        self.moon = self.world.CreateStaticBody(\n",
    "            shapes=edgeShape(vertices=[(0, 0), (W, 0)])\n",
    "        )\n",
    "        self.sky_polys = []\n",
    "        for i in range(CHUNKS - 1):\n",
    "            p1 = (chunk_x[i], smooth_y[i])\n",
    "            p2 = (chunk_x[i + 1], smooth_y[i + 1])\n",
    "            self.moon.CreateEdgeFixture(vertices=[p1, p2], density=0, friction=0.1)\n",
    "            self.sky_polys.append([p1, p2, (p2[0], H), (p1[0], H)])\n",
    "\n",
    "        self.moon.color1 = (0.0, 0.0, 0.0)\n",
    "        self.moon.color2 = (0.0, 0.0, 0.0)\n",
    "\n",
    "        initial_y = VIEWPORT_H / SCALE\n",
    "        self.lander = self.world.CreateDynamicBody(\n",
    "            position=(VIEWPORT_W / SCALE / 2, initial_y),\n",
    "            angle=0.0,\n",
    "            fixtures=fixtureDef(\n",
    "                shape=polygonShape(\n",
    "                    vertices=[(x / SCALE, y / SCALE) for x, y in LANDER_POLY]\n",
    "                ),\n",
    "                density=5.0,\n",
    "                friction=0.1,\n",
    "                categoryBits=0x0010,\n",
    "                maskBits=0x001,  # collide only with ground\n",
    "                restitution=0.0,\n",
    "            ),  # 0.99 bouncy\n",
    "        )\n",
    "        self.lander.color1 = (0.5, 0.4, 0.9)\n",
    "        self.lander.color2 = (0.3, 0.3, 0.5)\n",
    "        self.lander.ApplyForceToCenter(\n",
    "            (\n",
    "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
    "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
    "            ),\n",
    "            True,\n",
    "        )\n",
    "\n",
    "        self.legs = []\n",
    "        for i in [-1, +1]:\n",
    "            leg = self.world.CreateDynamicBody(\n",
    "                position=(VIEWPORT_W / SCALE / 2 - i * LEG_AWAY / SCALE, initial_y),\n",
    "                angle=(i * 0.05),\n",
    "                fixtures=fixtureDef(\n",
    "                    shape=polygonShape(box=(LEG_W / SCALE, LEG_H / SCALE)),\n",
    "                    density=1.0,\n",
    "                    restitution=0.0,\n",
    "                    categoryBits=0x0020,\n",
    "                    maskBits=0x001,\n",
    "                ),\n",
    "            )\n",
    "            leg.ground_contact = False\n",
    "            leg.color1 = (0.5, 0.4, 0.9)\n",
    "            leg.color2 = (0.3, 0.3, 0.5)\n",
    "            rjd = revoluteJointDef(\n",
    "                bodyA=self.lander,\n",
    "                bodyB=leg,\n",
    "                localAnchorA=(0, 0),\n",
    "                localAnchorB=(i * LEG_AWAY / SCALE, LEG_DOWN / SCALE),\n",
    "                enableMotor=True,\n",
    "                enableLimit=True,\n",
    "                maxMotorTorque=LEG_SPRING_TORQUE,\n",
    "                motorSpeed=+0.3 * i,  # low enough not to jump back into the sky\n",
    "            )\n",
    "            if i == -1:\n",
    "                rjd.lowerAngle = (\n",
    "                    +0.9 - 0.5\n",
    "                )  # The most esoteric numbers here, angled legs have freedom to travel within\n",
    "                rjd.upperAngle = +0.9\n",
    "            else:\n",
    "                rjd.lowerAngle = -0.9\n",
    "                rjd.upperAngle = -0.9 + 0.5\n",
    "            leg.joint = self.world.CreateJoint(rjd)\n",
    "            self.legs.append(leg)\n",
    "\n",
    "        self.drawlist = [self.lander] + self.legs\n",
    "\n",
    "        return self.step(np.array([0, 0]) if self.continuous else 0)[0]\n",
    "\n",
    "    def _create_particle(self, mass, x, y, ttl):\n",
    "        p = self.world.CreateDynamicBody(\n",
    "            position=(x, y),\n",
    "            angle=0.0,\n",
    "            fixtures=fixtureDef(\n",
    "                shape=circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
    "                density=mass,\n",
    "                friction=0.1,\n",
    "                categoryBits=0x0100,\n",
    "                maskBits=0x001,  # collide only with ground\n",
    "                restitution=0.3,\n",
    "            ),\n",
    "        )\n",
    "        p.ttl = ttl\n",
    "        self.particles.append(p)\n",
    "        self._clean_particles(False)\n",
    "        return p\n",
    "\n",
    "    def _clean_particles(self, all):\n",
    "        while self.particles and (all or self.particles[0].ttl < 0):\n",
    "            self.world.DestroyBody(self.particles.pop(0))\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.continuous:\n",
    "            action = np.clip(action, -1, +1).astype(np.float32)\n",
    "        else:\n",
    "            assert self.action_space.contains(\n",
    "                action\n",
    "            ), f\"{action!r} ({type(action)}) invalid \"\n",
    "\n",
    "        # Engines\n",
    "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
    "        side = (-tip[1], tip[0])\n",
    "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
    "\n",
    "        m_power = 0.0\n",
    "        if (self.continuous and action[0] > 0.0) or (\n",
    "            not self.continuous and action == 2\n",
    "        ):\n",
    "            # Main engine\n",
    "            if self.continuous:\n",
    "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
    "                assert m_power >= 0.5 and m_power <= 1.0\n",
    "            else:\n",
    "                m_power = 1.0\n",
    "            ox = (\n",
    "                tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
    "            )  # 4 is move a bit downwards, +-2 for randomness\n",
    "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
    "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
    "            p = self._create_particle(\n",
    "                3.5,  # 3.5 is here to make particle speed adequate\n",
    "                impulse_pos[0],\n",
    "                impulse_pos[1],\n",
    "                m_power,\n",
    "            )  # particles are just a decoration\n",
    "            p.ApplyLinearImpulse(\n",
    "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "            self.lander.ApplyLinearImpulse(\n",
    "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "\n",
    "        s_power = 0.0\n",
    "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
    "            not self.continuous and action in [1, 3]\n",
    "        ):\n",
    "            # Orientation engines\n",
    "            if self.continuous:\n",
    "                direction = np.sign(action[1])\n",
    "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
    "                assert s_power >= 0.5 and s_power <= 1.0\n",
    "            else:\n",
    "                direction = action - 2\n",
    "                s_power = 1.0\n",
    "            ox = tip[0] * dispersion[0] + side[0] * (\n",
    "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
    "            )\n",
    "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
    "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
    "            )\n",
    "            impulse_pos = (\n",
    "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
    "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
    "            )\n",
    "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
    "            p.ApplyLinearImpulse(\n",
    "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "            self.lander.ApplyLinearImpulse(\n",
    "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
    "                impulse_pos,\n",
    "                True,\n",
    "            )\n",
    "\n",
    "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
    "\n",
    "        pos = self.lander.position\n",
    "        vel = self.lander.linearVelocity\n",
    "        state = [\n",
    "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2),\n",
    "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2),\n",
    "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,\n",
    "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,\n",
    "            self.lander.angle,\n",
    "            20.0 * self.lander.angularVelocity / FPS,\n",
    "            1.0 if self.legs[0].ground_contact else 0.0,\n",
    "            1.0 if self.legs[1].ground_contact else 0.0,\n",
    "        ]\n",
    "        assert len(state) == 8\n",
    "\n",
    "        reward = 0\n",
    "        shaping = (\n",
    "            -100 * np.sqrt(state[0] * state[0] + state[1] * state[1])\n",
    "            - 100 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
    "            - 100 * abs(state[4])\n",
    "            + 10 * state[6]\n",
    "            + 10 * state[7]\n",
    "        )  # And ten points for legs contact, the idea is if you\n",
    "        # lose contact again after landing, you get negative reward\n",
    "        if self.prev_shaping is not None:\n",
    "            reward = shaping - self.prev_shaping\n",
    "        self.prev_shaping = shaping\n",
    "\n",
    "        reward -= (\n",
    "            m_power * 0.30\n",
    "        )  # less fuel spent is better, about -30 for heuristic landing\n",
    "        reward -= s_power * 0.03\n",
    "\n",
    "        done = False\n",
    "        if self.game_over or abs(state[0]) >= 1.0:\n",
    "            done = True\n",
    "            reward = -100\n",
    "        if not self.lander.awake:\n",
    "            done = True\n",
    "            reward = +100\n",
    "        return np.array(state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        from gym.utils import pyglet_rendering\n",
    "\n",
    "        if self.viewer is None:\n",
    "            self.viewer = pyglet_rendering.Viewer(VIEWPORT_W, VIEWPORT_H)\n",
    "            self.viewer.set_bounds(0, VIEWPORT_W / SCALE, 0, VIEWPORT_H / SCALE)\n",
    "\n",
    "        for obj in self.particles:\n",
    "            obj.ttl -= 0.15\n",
    "            obj.color1 = (\n",
    "                max(0.2, 0.2 + obj.ttl),\n",
    "                max(0.2, 0.5 * obj.ttl),\n",
    "                max(0.2, 0.5 * obj.ttl),\n",
    "            )\n",
    "            obj.color2 = (\n",
    "                max(0.2, 0.2 + obj.ttl),\n",
    "                max(0.2, 0.5 * obj.ttl),\n",
    "                max(0.2, 0.5 * obj.ttl),\n",
    "            )\n",
    "\n",
    "        self._clean_particles(False)\n",
    "\n",
    "        for p in self.sky_polys:\n",
    "            self.viewer.draw_polygon(p, color=(0, 0, 0))\n",
    "\n",
    "        for obj in self.particles + self.drawlist:\n",
    "            for f in obj.fixtures:\n",
    "                trans = f.body.transform\n",
    "                if type(f.shape) is circleShape:\n",
    "                    t = pyglet_rendering.Transform(translation=trans * f.shape.pos)\n",
    "                    self.viewer.draw_circle(\n",
    "                        f.shape.radius, 20, color=obj.color1\n",
    "                    ).add_attr(t)\n",
    "                    self.viewer.draw_circle(\n",
    "                        f.shape.radius, 20, color=obj.color2, filled=False, linewidth=2\n",
    "                    ).add_attr(t)\n",
    "                else:\n",
    "                    path = [trans * v for v in f.shape.vertices]\n",
    "                    self.viewer.draw_polygon(path, color=obj.color1)\n",
    "                    path.append(path[0])\n",
    "                    self.viewer.draw_polyline(path, color=obj.color2, linewidth=2)\n",
    "\n",
    "        for x in [self.helipad_x1, self.helipad_x2]:\n",
    "            flagy1 = self.helipad_y\n",
    "            flagy2 = flagy1 + 50 / SCALE\n",
    "            self.viewer.draw_polyline([(x, flagy1), (x, flagy2)], color=(1, 1, 1))\n",
    "            self.viewer.draw_polygon(\n",
    "                [\n",
    "                    (x, flagy2),\n",
    "                    (x, flagy2 - 10 / SCALE),\n",
    "                    (x + 25 / SCALE, flagy2 - 5 / SCALE),\n",
    "                ],\n",
    "                color=(0.8, 0.8, 0),\n",
    "            )\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "\n",
    "def heuristic(env, s):\n",
    "    \"\"\"\n",
    "    The heuristic for\n",
    "    1. Testing\n",
    "    2. Demonstration rollout.\n",
    "\n",
    "    Args:\n",
    "        env: The environment\n",
    "        s (list): The state. Attributes:\n",
    "                  s[0] is the horizontal coordinate\n",
    "                  s[1] is the vertical coordinate\n",
    "                  s[2] is the horizontal speed\n",
    "                  s[3] is the vertical speed\n",
    "                  s[4] is the angle\n",
    "                  s[5] is the angular speed\n",
    "                  s[6] 1 if first leg has contact, else 0\n",
    "                  s[7] 1 if second leg has contact, else 0\n",
    "    returns:\n",
    "         a: The heuristic to be fed into the step function defined above to determine the next step and reward.\n",
    "    \"\"\"\n",
    "\n",
    "    angle_targ = s[0] * 0.5 + s[2] * 1.0  # angle should point towards center\n",
    "    if angle_targ > 0.4:\n",
    "        angle_targ = 0.4  # more than 0.4 radians (22 degrees) is bad\n",
    "    if angle_targ < -0.4:\n",
    "        angle_targ = -0.4\n",
    "    hover_targ = 0.55 * np.abs(\n",
    "        s[0]\n",
    "    )  # target y should be proportional to horizontal offset\n",
    "\n",
    "    angle_todo = (angle_targ - s[4]) * 0.5 - (s[5]) * 1.0\n",
    "    hover_todo = (hover_targ - s[1]) * 0.5 - (s[3]) * 0.5\n",
    "\n",
    "    if s[6] or s[7]:  # legs have contact\n",
    "        angle_todo = 0\n",
    "        hover_todo = (\n",
    "            -(s[3]) * 0.5\n",
    "        )  # override to reduce fall speed, that's all we need after contact\n",
    "\n",
    "    if env.continuous:\n",
    "        a = np.array([hover_todo * 20 - 1, -angle_todo * 20])\n",
    "        a = np.clip(a, -1, +1)\n",
    "    else:\n",
    "        a = 0\n",
    "        if hover_todo > np.abs(angle_todo) and hover_todo > 0.05:\n",
    "            a = 2\n",
    "        elif angle_todo < -0.05:\n",
    "            a = 3\n",
    "        elif angle_todo > +0.05:\n",
    "            a = 1\n",
    "    return a\n",
    "\n",
    "\n",
    "def demo_heuristic_lander(env, seed=None, render=False):\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    s = env.reset(seed=seed)\n",
    "    while True:\n",
    "        a = heuristic(env, s)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total_reward += r\n",
    "\n",
    "        if render:\n",
    "            still_open = env.render()\n",
    "            if still_open == False:\n",
    "                break\n",
    "\n",
    "        if steps % 20 == 0 or done:\n",
    "            print(\"observations:\", \" \".join([f\"{x:+0.2f}\" for x in s]))\n",
    "            print(f\"step {steps} total_reward {total_reward:+0.2f}\")\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break\n",
    "    if render:\n",
    "        env.close()\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "class LunarLanderContinuous:\n",
    "    def __init__(self):\n",
    "        raise error.Error(\n",
    "            \"Error initializing LunarLanderContinuous Environment.\\n\"\n",
    "            \"Currently, we do not support initializing this mode of environment by calling the class directly.\\n\"\n",
    "            \"To use this environment, instead create it by specifying the continuous keyword in gym.make, i.e.\\n\"\n",
    "            'gym.make(\"LunarLander-v2\", continuous=True)'\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_heuristic_lander(LunarLander(), render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  \n",
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())  # take a random action\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "UnregisteredEnv",
     "evalue": "No registered env with id: Pong-v0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Pong-v0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jr/017d8q8145z52wbhxmqqxxkc0000gn/T/ipykernel_72390/1552942716.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pong-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mprev_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# used in computing the difference frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Making new env: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/tf2.6/lib/python3.9/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 )\n\u001b[1;32m    202\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnregisteredEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No registered env with id: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m: No registered env with id: Pong-v0"
     ]
    }
   ],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200  # number of hidden layer neurons\n",
    "batch_size = 10  # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99  # discount factor for reward\n",
    "decay_rate = 0.99  # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False  # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80  # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "  model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "  model = {}\n",
    "  model['W1'] = np.random.randn(H, D) / np.sqrt(D)  # \"Xavier\" initialization\n",
    "  model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "# update buffers that add up gradients over a batch\n",
    "grad_buffer = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "rmsprop_cache = {k: np.zeros_like(v)\n",
    "                 for k, v in model.items()}  # rmsprop memory\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "  # sigmoid \"squashing\" function to interval [0,1]\n",
    "  return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195]  # crop\n",
    "  I = I[::2, ::2, 0]  # downsample by factor of 2\n",
    "  I[I == 144] = 0  # erase background (background type 1)\n",
    "  I[I == 109] = 0  # erase background (background type 2)\n",
    "  I[I != 0] = 1  # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(xrange(0, r.size)):\n",
    "    if r[t] != 0:\n",
    "      # reset the sum, since this was a game boundary (pong specific!)\n",
    "      running_add = 0\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h < 0] = 0  # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h  # return probability of taking action 2, and hidden state\n",
    "\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0  # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1': dW1, 'W2': dW2}\n",
    "\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None  # used in computing the difference frame\n",
    "xs, hs, dlogps, drs = [], [], [], []\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "  if render:\n",
    "    env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, h = policy_forward(x)\n",
    "  action = 2 if np.random.uniform() < aprob else 3  # roll the dice!\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x)  # observation\n",
    "  hs.append(h)  # hidden state\n",
    "  y = 1 if action == 2 else 0  # a \"fake label\"\n",
    "  # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "  dlogps.append(y - aprob)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "\n",
    "  # record reward (has to be done after we call step() to get reward for previous action)\n",
    "  drs.append(reward)\n",
    "\n",
    "  if done:  # an episode finished\n",
    "    episode_number += 1\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    eph = np.vstack(hs)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    xs, hs, dlogps, drs = [], [], [], []  # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    epdlogp *= discounted_epr\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model:\n",
    "      grad_buffer[k] += grad[k]  # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for k, v in model.iteritems():\n",
    "        g = grad_buffer[k]  # gradient\n",
    "        rmsprop_cache[k] = decay_rate * \\\n",
    "            rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v)  # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * \\\n",
    "        0.99 + reward_sum * 0.01\n",
    "    print(f'resetting env. episode reward total was {reward_sum}. running mean: {running_reward}')\n",
    "    if episode_number % 100 == 0:\n",
    "      pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset()  # reset env\n",
    "    prev_x = None\n",
    "\n",
    "  if reward != 0:  # Pong has either +1 or -1 reward exactly when game ends.\n",
    "    print('ep %d: game finished, reward: %f' %\n",
    "          (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v0\n",
      "CartPole-v1\n",
      "MountainCar-v0\n",
      "MountainCarContinuous-v0\n",
      "Pendulum-v1\n",
      "Acrobot-v1\n",
      "LunarLander-v2\n",
      "LunarLanderContinuous-v2\n",
      "BipedalWalker-v3\n",
      "BipedalWalkerHardcore-v3\n",
      "CarRacing-v0\n",
      "Blackjack-v1\n",
      "FrozenLake-v1\n",
      "FrozenLake8x8-v1\n",
      "CliffWalking-v0\n",
      "Taxi-v3\n",
      "Reacher-v2\n",
      "Pusher-v2\n",
      "Thrower-v2\n",
      "Striker-v2\n",
      "InvertedPendulum-v2\n",
      "InvertedDoublePendulum-v2\n",
      "HalfCheetah-v2\n",
      "HalfCheetah-v3\n",
      "Hopper-v2\n",
      "Hopper-v3\n",
      "Swimmer-v2\n",
      "Swimmer-v3\n",
      "Walker2d-v2\n",
      "Walker2d-v3\n",
      "Ant-v2\n",
      "Ant-v3\n",
      "Humanoid-v2\n",
      "Humanoid-v3\n",
      "HumanoidStandup-v2\n",
      "FetchSlide-v1\n",
      "FetchPickAndPlace-v1\n",
      "FetchReach-v1\n",
      "FetchPush-v1\n",
      "HandReach-v0\n",
      "HandManipulateBlockRotateZ-v0\n",
      "HandManipulateBlockRotateZTouchSensors-v0\n",
      "HandManipulateBlockRotateZTouchSensors-v1\n",
      "HandManipulateBlockRotateParallel-v0\n",
      "HandManipulateBlockRotateParallelTouchSensors-v0\n",
      "HandManipulateBlockRotateParallelTouchSensors-v1\n",
      "HandManipulateBlockRotateXYZ-v0\n",
      "HandManipulateBlockRotateXYZTouchSensors-v0\n",
      "HandManipulateBlockRotateXYZTouchSensors-v1\n",
      "HandManipulateBlockFull-v0\n",
      "HandManipulateBlock-v0\n",
      "HandManipulateBlockTouchSensors-v0\n",
      "HandManipulateBlockTouchSensors-v1\n",
      "HandManipulateEggRotate-v0\n",
      "HandManipulateEggRotateTouchSensors-v0\n",
      "HandManipulateEggRotateTouchSensors-v1\n",
      "HandManipulateEggFull-v0\n",
      "HandManipulateEgg-v0\n",
      "HandManipulateEggTouchSensors-v0\n",
      "HandManipulateEggTouchSensors-v1\n",
      "HandManipulatePenRotate-v0\n",
      "HandManipulatePenRotateTouchSensors-v0\n",
      "HandManipulatePenRotateTouchSensors-v1\n",
      "HandManipulatePenFull-v0\n",
      "HandManipulatePen-v0\n",
      "HandManipulatePenTouchSensors-v0\n",
      "HandManipulatePenTouchSensors-v1\n",
      "FetchSlideDense-v1\n",
      "FetchPickAndPlaceDense-v1\n",
      "FetchReachDense-v1\n",
      "FetchPushDense-v1\n",
      "HandReachDense-v0\n",
      "HandManipulateBlockRotateZDense-v0\n",
      "HandManipulateBlockRotateZTouchSensorsDense-v0\n",
      "HandManipulateBlockRotateZTouchSensorsDense-v1\n",
      "HandManipulateBlockRotateParallelDense-v0\n",
      "HandManipulateBlockRotateParallelTouchSensorsDense-v0\n",
      "HandManipulateBlockRotateParallelTouchSensorsDense-v1\n",
      "HandManipulateBlockRotateXYZDense-v0\n",
      "HandManipulateBlockRotateXYZTouchSensorsDense-v0\n",
      "HandManipulateBlockRotateXYZTouchSensorsDense-v1\n",
      "HandManipulateBlockFullDense-v0\n",
      "HandManipulateBlockDense-v0\n",
      "HandManipulateBlockTouchSensorsDense-v0\n",
      "HandManipulateBlockTouchSensorsDense-v1\n",
      "HandManipulateEggRotateDense-v0\n",
      "HandManipulateEggRotateTouchSensorsDense-v0\n",
      "HandManipulateEggRotateTouchSensorsDense-v1\n",
      "HandManipulateEggFullDense-v0\n",
      "HandManipulateEggDense-v0\n",
      "HandManipulateEggTouchSensorsDense-v0\n",
      "HandManipulateEggTouchSensorsDense-v1\n",
      "HandManipulatePenRotateDense-v0\n",
      "HandManipulatePenRotateTouchSensorsDense-v0\n",
      "HandManipulatePenRotateTouchSensorsDense-v1\n",
      "HandManipulatePenFullDense-v0\n",
      "HandManipulatePenDense-v0\n",
      "HandManipulatePenTouchSensorsDense-v0\n",
      "HandManipulatePenTouchSensorsDense-v1\n",
      "CubeCrash-v0\n",
      "CubeCrashSparse-v0\n",
      "CubeCrashScreenBecomesBlack-v0\n",
      "MemorizeDigits-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "for i in gym.envs.registry.all():\n",
    "  print(i.id)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1540c76d30b1af6ad410dda6d61bebe77544e2f3c1a272e4083818637a64b89a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tf2.6': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
