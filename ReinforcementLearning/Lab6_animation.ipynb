{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EPOCHS = 1000\n",
    "THRESHOLD = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\p2004176\\AppData\\Local\\Temp/ipykernel_17540/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|State|Action|Reward|Done|\n",
    "|---|---|---|---|\n",
    "|$(x, v, \\theta, \\omega)$| {0, 1} | {0, 1} | {0, 1} |\n",
    "|$x$: position | 0: push to the left | 0: game ended | 0: game continues |\n",
    "|$v$: velocity | 1: push to the right | 1: game continues | 1: game ended |\n",
    "|$\\theta$: angle |  | | |\n",
    "|$\\omega$: angular velocity | | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, env_string, batch_size=64):\n",
    "        self.memory = deque(maxlen=100000)  # deque is similar to a list, but provide faster append and pop operation.\n",
    "        self.env = gym.make(env_string)  # create the simulation environment\n",
    "        input_size = self.env.observation_space.shape[0]  # the state of the agent (position, velocity, angle, angular velocity)\n",
    "        action_size = self.env.action_space.n  # how many possible actions can the agent take, in this example, it is 2.\n",
    "        self.batch_size = batch_size  # batch size of training data\n",
    "        self.gamma = 1.0  # gamma is the discount factor\n",
    "        # epsilon determine the ratio of exploration, the higher the epsilon, the more exploration the agent will do\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        alpha=0.01  # alpha is learning rate\n",
    "        alpha_decay=0.01\n",
    "        \n",
    "        # Initialize Deep Q network model (the network is to predict the cumulative value given a certain state)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=input_size, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=alpha, decay=alpha_decay))\n",
    "    \n",
    "\n",
    "    # Store past experience (S,A,R,S') into the memory, which will be served as training data to the neural network\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "\n",
    "    # choose random action to explore the environment, or exploit the action with the highest value\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "    \n",
    "\n",
    "    # Preprocess: reshape the state into 2D array\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "    \n",
    "    \n",
    "    # Replay: randomly select a batch of training data (S,A,R,S') from the agent's previous interaction \n",
    "    # with the environment and this can solve the oscilation or divergence of network weights problem.\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)  # y_target is the predicted Q value for each action\n",
    "            # y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            if done:\n",
    "                y_target[0][action] = reward\n",
    "            else:\n",
    "                y_target[0][action] = reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        #epsilon = max(epsilon_min, epsilon_decay*epsilon) # decrease epsilon\n",
    "       \n",
    "\n",
    "    def train(self):\n",
    "        scores = deque(maxlen=100)\n",
    "        avg_scores = []\n",
    "        \n",
    "        for e in range(EPOCHS):\n",
    "            state = self.env.reset()\n",
    "            state = self.preprocess_state(state)\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:  # when the pole falls down, the current epoch ends and a new epoch will start\n",
    "                self.env.render()\n",
    "                action = self.choose_action(state,self.epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon_decay*self.epsilon) # decrease epsilon\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = round(np.mean(scores), 1)\n",
    "            avg_scores.append(mean_score)\n",
    "            if mean_score >= THRESHOLD and e >= 100:\n",
    "                print('Ran {} episodes. Solved after {} trials, averaging a score of {} ticks. âœ”'.format(e, e - 100, mean_score))\n",
    "                return avg_scores\n",
    "            if e % 10 == 0:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        print('Did not solve after {} episodes ðŸ˜ž'.format(e+1))\n",
    "        return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 25.0 ticks.\n",
      "[Episode 10] - Mean survival time over last 100 episodes was 33.9 ticks.\n",
      "[Episode 20] - Mean survival time over last 100 episodes was 22.9 ticks.\n",
      "[Episode 30] - Mean survival time over last 100 episodes was 18.7 ticks.\n",
      "[Episode 40] - Mean survival time over last 100 episodes was 17.5 ticks.\n",
      "[Episode 50] - Mean survival time over last 100 episodes was 22.3 ticks.\n",
      "[Episode 60] - Mean survival time over last 100 episodes was 21.2 ticks.\n",
      "[Episode 70] - Mean survival time over last 100 episodes was 20.7 ticks.\n",
      "[Episode 80] - Mean survival time over last 100 episodes was 21.7 ticks.\n",
      "[Episode 90] - Mean survival time over last 100 episodes was 21.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 20.9 ticks.\n",
      "[Episode 110] - Mean survival time over last 100 episodes was 19.1 ticks.\n",
      "[Episode 120] - Mean survival time over last 100 episodes was 19.7 ticks.\n",
      "[Episode 130] - Mean survival time over last 100 episodes was 23.4 ticks.\n",
      "[Episode 140] - Mean survival time over last 100 episodes was 27.1 ticks.\n",
      "[Episode 150] - Mean survival time over last 100 episodes was 26.1 ticks.\n",
      "[Episode 160] - Mean survival time over last 100 episodes was 29.8 ticks.\n",
      "[Episode 170] - Mean survival time over last 100 episodes was 36.6 ticks.\n",
      "[Episode 180] - Mean survival time over last 100 episodes was 48.4 ticks.\n",
      "[Episode 190] - Mean survival time over last 100 episodes was 51.3 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 51.5 ticks.\n",
      "[Episode 210] - Mean survival time over last 100 episodes was 51.5 ticks.\n",
      "[Episode 220] - Mean survival time over last 100 episodes was 52.6 ticks.\n",
      "[Episode 230] - Mean survival time over last 100 episodes was 52.8 ticks.\n",
      "[Episode 240] - Mean survival time over last 100 episodes was 51.4 ticks.\n",
      "[Episode 250] - Mean survival time over last 100 episodes was 50.1 ticks.\n",
      "[Episode 260] - Mean survival time over last 100 episodes was 46.1 ticks.\n",
      "[Episode 270] - Mean survival time over last 100 episodes was 38.5 ticks.\n",
      "[Episode 280] - Mean survival time over last 100 episodes was 25.1 ticks.\n",
      "[Episode 290] - Mean survival time over last 100 episodes was 23.1 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 22.2 ticks.\n",
      "[Episode 310] - Mean survival time over last 100 episodes was 21.6 ticks.\n",
      "[Episode 320] - Mean survival time over last 100 episodes was 19.8 ticks.\n",
      "[Episode 330] - Mean survival time over last 100 episodes was 16.0 ticks.\n",
      "[Episode 340] - Mean survival time over last 100 episodes was 19.1 ticks.\n",
      "[Episode 350] - Mean survival time over last 100 episodes was 29.6 ticks.\n",
      "[Episode 360] - Mean survival time over last 100 episodes was 33.0 ticks.\n",
      "[Episode 370] - Mean survival time over last 100 episodes was 33.4 ticks.\n",
      "[Episode 380] - Mean survival time over last 100 episodes was 33.8 ticks.\n",
      "[Episode 390] - Mean survival time over last 100 episodes was 33.4 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 46.2 ticks.\n",
      "[Episode 410] - Mean survival time over last 100 episodes was 53.7 ticks.\n",
      "[Episode 420] - Mean survival time over last 100 episodes was 58.8 ticks.\n",
      "[Episode 430] - Mean survival time over last 100 episodes was 59.8 ticks.\n",
      "[Episode 440] - Mean survival time over last 100 episodes was 54.5 ticks.\n",
      "[Episode 450] - Mean survival time over last 100 episodes was 44.3 ticks.\n",
      "[Episode 460] - Mean survival time over last 100 episodes was 51.7 ticks.\n",
      "[Episode 470] - Mean survival time over last 100 episodes was 54.7 ticks.\n",
      "[Episode 480] - Mean survival time over last 100 episodes was 57.7 ticks.\n",
      "[Episode 490] - Mean survival time over last 100 episodes was 64.8 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 56.8 ticks.\n",
      "[Episode 510] - Mean survival time over last 100 episodes was 55.1 ticks.\n",
      "[Episode 520] - Mean survival time over last 100 episodes was 65.6 ticks.\n",
      "[Episode 530] - Mean survival time over last 100 episodes was 76.7 ticks.\n",
      "[Episode 540] - Mean survival time over last 100 episodes was 88.3 ticks.\n",
      "[Episode 550] - Mean survival time over last 100 episodes was 100.3 ticks.\n",
      "[Episode 560] - Mean survival time over last 100 episodes was 98.5 ticks.\n",
      "[Episode 570] - Mean survival time over last 100 episodes was 102.8 ticks.\n",
      "[Episode 580] - Mean survival time over last 100 episodes was 106.8 ticks.\n",
      "[Episode 590] - Mean survival time over last 100 episodes was 107.5 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 120.0 ticks.\n",
      "[Episode 610] - Mean survival time over last 100 episodes was 132.8 ticks.\n",
      "[Episode 620] - Mean survival time over last 100 episodes was 129.9 ticks.\n",
      "[Episode 630] - Mean survival time over last 100 episodes was 133.2 ticks.\n",
      "[Episode 640] - Mean survival time over last 100 episodes was 129.8 ticks.\n",
      "[Episode 650] - Mean survival time over last 100 episodes was 124.6 ticks.\n",
      "[Episode 660] - Mean survival time over last 100 episodes was 123.5 ticks.\n",
      "[Episode 670] - Mean survival time over last 100 episodes was 126.4 ticks.\n",
      "[Episode 680] - Mean survival time over last 100 episodes was 131.2 ticks.\n",
      "[Episode 690] - Mean survival time over last 100 episodes was 135.6 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 129.3 ticks.\n",
      "[Episode 710] - Mean survival time over last 100 episodes was 123.4 ticks.\n",
      "[Episode 720] - Mean survival time over last 100 episodes was 122.2 ticks.\n",
      "[Episode 730] - Mean survival time over last 100 episodes was 119.1 ticks.\n",
      "[Episode 740] - Mean survival time over last 100 episodes was 121.6 ticks.\n",
      "[Episode 750] - Mean survival time over last 100 episodes was 126.0 ticks.\n",
      "[Episode 760] - Mean survival time over last 100 episodes was 135.2 ticks.\n",
      "Ran 766 episodes. Solved after 666 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "env_string = 'CartPole-v1'\n",
    "agent = DQN(env_string)\n",
    "scores = agent.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "417880eb038c49c346aaca12af5a57d5875e15532e722506ed6731d77d3493bc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
