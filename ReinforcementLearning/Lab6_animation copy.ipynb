{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\__main__.py\", line 16, in <module>\n",
      "    from pip._internal.cli.main import main as _main  # isort:skip # noqa\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\main.py\", line 10, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\autocompletion.py\", line 9, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\main_parser.py\", line 7, in <module>\n",
      "    from pip._internal.cli import cmdoptions\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\cmdoptions.py\", line 28, in <module>\n",
      "    from pip._internal.models.target_python import TargetPython\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\models\\target_python.py\", line 4, in <module>\n",
      "    from pip._internal.utils.misc import normalize_version_info\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 20, in <module>\n",
      "    from pip._vendor import pkg_resources\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3251, in <module>\n",
      "    @_call_aside\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3235, in _call_aside\n",
      "    f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3264, in _initialize_master_working_set\n",
      "    working_set = WorkingSet._build_master()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 574, in _build_master\n",
      "    ws = cls()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 567, in __init__\n",
      "    self.add_entry(entry)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 623, in add_entry\n",
      "    for dist in find_distributions(entry, True):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2065, in find_on_path\n",
      "    for dist in factory(fullpath):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2135, in distributions_from_metadata\n",
      "    root, entry, metadata, precedence=DEVELOP_DIST,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2592, in from_location\n",
      "    py_version=py_version, platform=platform, **kw\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2994, in _reload_version\n",
      "    md_version = self._get_version()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2772, in _get_version\n",
      "    version = _version_from_file(lines)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2556, in _version_from_file\n",
      "    line = next(iter(version_lines), '')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2767, in _get_metadata\n",
      "    for line in self.get_metadata_lines(name):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1432, in get_metadata_lines\n",
      "    return yield_lines(self.get_metadata(name))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "PermissionError: [Errno 13] Permission denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\torchvision-0.8.2-py3.7.egg-info\\\\PKG-INFO'\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-c5c2bf454da6>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mrandom\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mgym\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mcollections\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdeque\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EPOCHS = 1000\n",
    "THRESHOLD = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|State|Action|Reward|Done|\n",
    "|---|---|---|---|\n",
    "|$(x, v, \\theta, \\omega)$| {0, 1} | {0, 1} | {0, 1} |\n",
    "|$x$: position | 0: push to the left | 0: game ended | 0: game continues |\n",
    "|$v$: velocity | 1: push to the right | 1: game continues | 1: game ended |\n",
    "|$\\theta$: angle |  | | |\n",
    "|$\\omega$: angular velocity | | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, env_string, batch_size=64):\n",
    "        self.memory = deque(maxlen=100000)  # deque is similar to a list, but provide faster append and pop operation.\n",
    "        self.env = gym.make(env_string)  # create the simulation environment\n",
    "        input_size = self.env.observation_space.shape[0]  # the state of the agent (position, velocity, angle, angular velocity)\n",
    "        action_size = self.env.action_space.n  # how many possible actions can the agent take, in this example, it is 2.\n",
    "        self.batch_size = batch_size  # batch size of training data\n",
    "        self.gamma = 1.0  # gamma is the discount factor\n",
    "        # epsilon determine the ratio of exploration, the higher the epsilon, the more exploration the agent will do\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        alpha=0.01  # alpha is learning rate\n",
    "        alpha_decay=0.01\n",
    "        \n",
    "        # Initialize Deep Q network model (the network is to predict the cumulative value given a certain state)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=input_size, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=alpha, decay=alpha_decay))\n",
    "    \n",
    "\n",
    "    # Store past experience (S,A,R,S') into the memory, which will be served as training data to the neural network\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "\n",
    "    # choose random action to explore the environment, or exploit the action with the highest value\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "    \n",
    "\n",
    "    # Preprocess: reshape the state into 2D array\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "    \n",
    "    \n",
    "    # Replay: randomly select a batch of training data (S,A,R,S') from the agent's previous interaction \n",
    "    # with the environment and this can solve the oscilation or divergence of network weights problem.\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)  # y_target is the predicted Q value for each action\n",
    "            # y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            if done:\n",
    "                y_target[0][action] = reward\n",
    "            else:\n",
    "                y_target[0][action] = reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        #epsilon = max(epsilon_min, epsilon_decay*epsilon) # decrease epsilon\n",
    "       \n",
    "\n",
    "    def train(self):\n",
    "        scores = deque(maxlen=100)\n",
    "        avg_scores = []\n",
    "        \n",
    "        for e in range(EPOCHS):\n",
    "            state = self.env.reset()\n",
    "            state = self.preprocess_state(state)\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:  # when the pole falls down, the current epoch ends and a new epoch will start\n",
    "                self.env.render()\n",
    "                action = self.choose_action(state,self.epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon_decay*self.epsilon) # decrease epsilon\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = round(np.mean(scores), 1)\n",
    "            avg_scores.append(mean_score)\n",
    "            if mean_score >= THRESHOLD and e >= 100:\n",
    "                print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return avg_scores\n",
    "            if e % 10 == 0:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        print('Did not solve after {} episodes ðŸ˜ž'.format(e+1))\n",
    "        return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_string = 'CartPole-v1'\n",
    "agent = DQN(env_string)\n",
    "scores = agent.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "417880eb038c49c346aaca12af5a57d5875e15532e722506ed6731d77d3493bc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}