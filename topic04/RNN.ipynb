{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "# Set the random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imdb data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,) (50000,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes [0 1]\n",
      "Number of words 88585\n",
      "Mean 234.75892 words 172.91149458735703\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#EDA\n",
    "\n",
    "print(\"Classes\", np.unique(y))\n",
    "print(\"Number of words\", len(np.unique(np.hstack(X))))\n",
    "result = [len(x) for x in X]\n",
    "print(f\"Mean {np.mean(result)} words {np.std(result)}\")\n",
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding/truncate the sewntences\n",
    "max_words = 500\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               4096256   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,256,513\n",
      "Trainable params: 4,256,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(5000, 32, input_length=max_words),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Attempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[node sequential_1/dense_2/MatMul (defined at \\AppData\\Local\\Temp/ipykernel_18652/497418377.py:1) ]] [Op:__inference_train_function_1620]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18652/497418377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\DELE\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DELE\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DELE\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DELE\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DELE\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\.conda\\envs\\DELE\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DELE\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Attempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[node sequential_1/dense_2/MatMul (defined at \\AppData\\Local\\Temp/ipykernel_18652/497418377.py:1) ]] [Op:__inference_train_function_1620]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n",
      "17473536/17464789 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 15:04:13.587450: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 15:04:14.375626: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 - 7s - loss: 0.4833 - accuracy: 0.7390 - val_loss: 0.2896 - val_accuracy: 0.8768\n",
      "Epoch 2/2\n",
      "196/196 - 6s - loss: 0.1813 - accuracy: 0.9319 - val_loss: 0.3388 - val_accuracy: 0.8612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f92ad6b8220>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.12%\n"
     ]
    }
   ],
   "source": [
    "# MLP for the IMDB problem\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "# numpy.random.seed(1)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(\n",
    "    X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 153s 388ms/step - loss: 0.4571 - accuracy: 0.7780 - val_loss: 0.3517 - val_accuracy: 0.8546\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 150s 385ms/step - loss: 0.3504 - accuracy: 0.8583 - val_loss: 0.3209 - val_accuracy: 0.8678\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 140s 359ms/step - loss: 0.2636 - accuracy: 0.8968 - val_loss: 0.3261 - val_accuracy: 0.8656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f92cc65d4c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.56%\n"
     ]
    }
   ],
   "source": [
    "# LSTM for the IMDB problem\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "# numpy.random.seed(1)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length,\n",
    "                    input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3,\n",
    "          batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d817ecc40216b7f0162e924e851aea10644fd94c62e64bd88718080ddb5aebae"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('DELE': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
